---
---


@inproceedings{kovatchev-etal-2018-warp,
    title = "{WARP}-Text: a Web-Based Tool for Annotating Relationships between Pairs of Texts",
    author = "Kovatchev, Venelin  and
      Mart{\'\i}, M. Ant{\`o}nia  and
      Salam{\'o}, Maria",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-2029",
    pages = "132--136",
    abstract = "We present WARP-Text, an open-source web-based tool for annotating relationships between pairs of texts. WARP-Text supports multi-layer annotation and custom definitions of inter-textual and intra-textual relationships. Annotation can be performed at different granularity levels (such as sentences, phrases, or tokens). WARP-Text has an intuitive user-friendly interface both for project managers and annotators. WARP-Text fills a gap in the currently available NLP toolbox, as open-source alternatives for annotation of pairs of text are not readily available. WARP-Text has already been used in several annotation tasks and can be of interest to the researchers working in the areas of Paraphrasing, Entailment, Simplification, and Summarization, among others.",
}

@inproceedings{hossain-etal-2020-analysis,
    title = "An Analysis of Natural Language Inference Benchmarks through the Lens of Negation",
    author = "Hossain, Md Mosharaf  and
      Kovatchev, Venelin  and
      Dutta, Pranoy  and
      Kao, Tiffany  and
      Wei, Elizabeth  and
      Blanco, Eduardo",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.732",
    doi = "10.18653/v1/2020.emnlp-main.732",
    pages = "9106--9118",

}

@inproceedings{gold-etal-2019-annotating,
    title = "Annotating and analyzing the interactions between meaning relations",
    author = "Gold, Darina  and
      Kovatchev, Venelin  and
      Zesch, Torsten",
    booktitle = "Proceedings of the 13th Linguistic Annotation Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4004",
    doi = "10.18653/v1/W19-4004",
    pages = "26--36",

}

@inproceedings{kovatchev-etal-2018-etpc,
    title = "{ETPC} - A Paraphrase Identification Corpus Annotated with Extended Paraphrase Typology and Negation",
    author = "Kovatchev, Venelin  and
      Mart{\'\i}, M. Ant{\`o}nia  and
      Salam{\'o}, Maria",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1221",
}


@inproceedings{kovatchev-etal-2019-qualitative,
    title = "A Qualitative Evaluation Framework for Paraphrase Identification",
    author = "Kovatchev, Venelin  and
      Marti, M. Antonia  and
      Salamo, Maria  and
      Beltran, Javier",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R19-1067",
    doi = "10.26615/978-954-452-056-4_067",
    pages = "568--577",
    abstract = "In this paper, we present a new approach for the evaluation, error analysis, and interpretation of supervised and unsupervised Paraphrase Identification (PI) systems. Our evaluation framework makes use of a PI corpus annotated with linguistic phenomena to provide a better understanding and interpretation of the performance of various PI systems. Our approach allows for a qualitative evaluation and comparison of the PI models using human interpretable categories. It does not require modification of the training objective of the systems and does not place additional burden on the developers. We replicate several popular supervised and unsupervised PI systems. Using our evaluation framework we show that: 1) Each system performs differently with respect to a set of linguistic phenomena and makes qualitatively different kinds of errors; 2) Some linguistic phenomena are more challenging than others across all systems.",
}

@inproceedings{kovatchev-etal-2020-decomposing,
    title = "Decomposing and Comparing Meaning Relations: Paraphrasing, Textual Entailment, Contradiction, and Specificity",
    author = "Kovatchev, Venelin  and
      Gold, Darina  and
      Marti, M. Antonia  and
      Salamo, Maria  and
      Zesch, Torsten",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.709",
    pages = "5782--5791",
    abstract = "In this paper, we present a methodology for decomposing and comparing multiple meaning relations (paraphrasing, textual entailment, contradiction, and specificity). The methodology includes SHARel - a new typology that consists of 26 linguistic and 8 reason-based categories. We use the typology to annotate a corpus of 520 sentence pairs in English and we demonstrate that unlike previous typologies, SHARel can be applied to all relations of interest with a high inter-annotator agreement. We analyze and compare the frequency and distribution of the linguistic and reason-based phenomena involved in paraphrasing, textual entailment, contradiction, and specificity. This comparison allows for a much more in-depth analysis of the workings of the individual relations and the way they interact and compare with each other. We release all resources (typology, annotation guidelines, and annotated corpus) to the community.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{kovatchev-etal-2022-longhorns,
    title = "longhorns at {DADC} 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks.",
    author = "Kovatchev, Venelin  and
      Chatterjee, Trina  and
      Govindarajan, Venkata S  and
      Chen, Jifan  and
      Choi, Eunsol  and
      Chronis, Gabriella  and
      Das, Anubrata  and
      Erk, Katrin  and
      Lease, Matthew  and
      Li, Junyi Jessy  and
      Wu, Yating  and
      Mahowald, Kyle",
    booktitle = "Proceedings of the First Workshop on Dynamic Adversarial Data Collection",
    month = jul,
    year = "2022",
    address = "Seattle, WA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dadc-1.5",
    doi = "10.18653/v1/2022.dadc-1.5",
    pages = "41--52",
    abstract = "Developing methods to adversarially challenge NLP systems is a promising avenue for improving both model performance and interpretability. Here, we describe the approach of the team {``}longhorns{''} on Task 1 of the The First Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to manually fool a model on an Extractive Question Answering task. Our team finished first (pending validation), with a model error rate of 62{\%}. We advocate for a systematic, linguistically informed approach to formulating adversarial questions, and we describe the results of our pilot experiments, as well as our official submission.",
}

@inproceedings{kovatchev2020your,
  title={“What is on your mind?” Automated Scoring of Mindreading in Childhood and Early Adolescence},
  author={Kovatchev, Venelin and Smith, Phillip and Lee, Mark and Traynor, Imogen Grumley and Aguilera, Irene Luque and Devine, Rory},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={6217--6228},
  year={2020}
}

@inproceedings{das2022prototex,
  title={ProtoTEx: Explaining Model Decisions with Prototype Tensors},
  author={Das, Anubrata and Gupta, Chitrank and Kovatchev, Venelin and Lease, Matthew and Li, Junyi Jessy},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2986--2997},
  year={2022}
}

@inproceedings{kovatchev2021can,
  title={Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children’s mindreading ability},
  author={Kovatchev, Venelin and Smith, Phillip and Lee, Mark and Devine, Rory},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1196--1206},
  year={2021}
}

@article{DAS2023103219,
title = {The state of human-centered NLP technology for fact-checking},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103219},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103219},
url = {https://www.sciencedirect.com/science/article/pii/S030645732200320X},
author = {Anubrata Das and Houjiang Liu and Venelin Kovatchev and Matthew Lease},
keywords = {Natural Language Processing, Misinformation, Disinformation, Explainability, Human-AI teaming},
abstract = {Misinformation threatens modern society by promoting distrust in science, changing narratives in public health, heightening social polarization, and disrupting democratic elections and financial markets, among a myriad of other societal harms. To address this, a growing cadre of professional fact-checkers and journalists provide high-quality investigations into purported facts. However, these largely manual efforts have struggled to match the enormous scale of the problem. In response, a growing body of Natural Language Processing (NLP) technologies have been proposed for more scalable fact-checking. Despite tremendous growth in such research, however, practical adoption of NLP technologies for fact-checking still remains in its infancy today. In this work, we review the capabilities and limitations of the current NLP technologies for fact-checking. Our particular focus is to further chart the design space for how these technologies can be harnessed and refined in order to better meet the needs of human fact-checkers. To do so, we review key aspects of NLP-based fact-checking: task formulation, dataset construction, modeling, and human-centered strategies, such as explainable models and human-in-the-loop approaches. Next, we review the efficacy of applying NLP-based fact-checking tools to assist human fact-checkers. We recommend that future research include collaboration with fact-checker stakeholders early on in NLP research, as well as incorporation of human-centered design practices in model development, in order to further guide technology development for human use and practical adoption. Finally, we advocate for more research on benchmark development supporting extrinsic evaluation of human-centered fact-checking technologies.}
}

@inproceedings{kovatchev-taule-2022-inferes,
    title = "{I}nfer{ES} : A Natural Language Inference Corpus for {S}panish Featuring Negation-Based Contrastive and Adversarial Examples",
    author = "Kovatchev, Venelin  and
      Taul{\'e}, Mariona",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.340",
    pages = "3873--3884",
    abstract = "In this paper we present InferES - an original corpus for Natural Language Inference (NLI) in European Spanish. We propose, implement, and analyze a variety of corpus-creating strategies utilizing expert linguists and crowd workers. The objectives behind InferES are to provide high-quality data, and at the same time to facilitate the systematic evaluation of automated systems. Specifically, we focus on measuring and improving the performance of machine learning systems on negation-based adversarial examples and their ability to generalize across out-of-distribution topics. We train two transformer models on InferES (8,055 gold examples) in a variety of scenarios. Our best model obtains 72.8{\%} accuracy, leaving a lot of room for improvement. The {``}hypothesis-only{''} baseline performs only 2{\%}-5{\%} higher than majority, indicating much fewer annotation artifacts than prior work. We show that models trained on InferES generalize very well across topics (both in- and out-of-distribution) and perform moderately well on negation-based adversarial examples.",
}